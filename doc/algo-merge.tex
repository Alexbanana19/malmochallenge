\documentclass[landscape, 8pt, a4paper]{article}

\usepackage{ctex}
\usepackage{clrscode3e}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{fancyhdr}

\geometry{top=1cm,bottom=1cm,left=1cm,right=1cm}
\DeclareMathOperator*{\argmax}{argmax}

\begin{document}

\columnseprule=1pt
\begin{multicols}{2}
\begin{center}

\textbf{\Large Offline Algorithm}

\begin{codebox}
	\Procname{$\proc{1.Collecting-Data}$}
	\zi $\kw{initialize}~~\proc{Replay-Buffer}~D$
	%\zi $\kw{initialize}~~\proc{Target-Action-Value-Function}(\theta^{-}=\theta)~\hat{Q}$
	\zi \For $episode\gets 1$ \To $M$ \Do
	\zi $\kw{initialize}~~\proc{Temp-Buffer}~G,~~\proc{State}~S,~~\proc{Behavioral-Policy}~\mu$
	\zi		\For  $step\gets 1$ \To $episode$ \Do
	\zi			$r,S'\gets\proc{transition}(S,\mu)$
	\zi			$\kw{store}~S~\kw{in}~G$
	\zi			$\proc{Goal}~g\gets S'$
	\zi			\kw{update}~$p$~\kw{with}~normalized-\proc{Behavior-Vector}
	\zi         	\For \id{state} \kw{in} $G$ \Do
	\zi				$\tilde{r}\gets \mbox{discount sum of reward}~\kw{from}~\id{state}~\kw{to}~S'$
	\zi				$\tilde{\gamma}\gets \gamma^{step}$
	\zi				$\tilde{S}\gets \proc{concatenate}(S,p)$
	\zi				$\tilde{S'}\gets\proc{concatenate}(S',p)$
	\zi				$\kw{store}~\proc{transition}(\tilde{S},g,\tilde{r},\tilde{S'})~\kw{in}~D$
	\End
	\End
	\zi	 		\kw{change}~\proc{Behavioral-Policy}~$\mu$~every~$C'$~steps
	\zi			\kw{resample} \kw{from} $p$ every $C'$ steps
	\End
	\End
\end{codebox}
%
\begin{codebox}
	\Procname{$\proc{2.Training}$}
	\zi $\kw{initialize}~~\proc{Action-Value-Function}(\theta)~Q$
	\zi $\kw{initialize}~~\proc{Target-Action-Value-Function}(\theta^{\mbox{{}-}}=\theta)~\hat{Q}$
	\zi \For $t\gets 1$ \To $T$ \Do
	\zi	\kw{sample} $\proc{Minibatch}<\tilde{S},g,\tilde{r},\tilde{S'},\tilde{\gamma}>$ \kw{from} $D$
	\zi $y=\tilde{r}+\tilde{\gamma}\cdot\max Q_{\theta}\left(\tilde{S'}, \argmax\limits_{g} Q_{\theta^{-}}(\tilde{S},g)\right)$
	\zi \kw{perform}~SGD~\kw{on}~$[y-Q_{\theta}(\tilde{S},g)]^2$ with respect to $\theta$
	\zi \kw{reset}~$\hat{Q}=Q$~every~$C$~steps
	\End
\end{codebox}

\textbf{\Large Online Algorithm}

\begin{codebox}
\zi \proc{Training}
\zi \> $\kw{initialize}~~\proc{Replay-Memory}(\id{capacity~N})~D$
\zi \> $\kw{initialize}~~\proc{Action-Value-Function}(\id{random~weight~\theta})~Q$
\zi \> $\kw{initialize}~~\proc{Target-Action-Value-Function}(\theta^{-}=\theta)~\hat{Q}$
\zi	\> \For $episode \gets 1$ \To $M$ \Do
\zi	\> 	$\kw{initialize}~~\proc{State}~S,~~\proc{Temp-Buffer}~G$
\zi	\> 	$\kw{initialize}~~\proc{Collabortor-Probability-Vector}(\id{Uniform})~p$
\zi	\> 	\For $step\gets 1$ \To $T$\Do
\zi	\> 			$\tilde{r}\gets \mbox{discount sum of reward \kw{from} \id{S} to }S'$
\zi	\> 			$\tilde{\gamma}\gets \gamma^{step}$
\zi	\> 			$\tilde{S}\gets \proc{Concatenate}(S,p)$
\zi	\> 			$\tilde{S'}\gets\proc{Concatenate}(S',p)$
\zi	\> 			$\kw{store}~\proc{transition}(\tilde{S},g,\tilde{r},\tilde{S'})~\kw{in}~D$
\zi	\> 			$a_t=\left\{\begin{array}{lc}
								\id{random~action} & \mbox{with probality}\,\epsilon \\
								\argmax\limits_a Q_{\theta}(s,g) & \mbox{otherwise}
					\end{array}\right.$
\zi	\> 			\kw{execute} $a_t$,~~\kw{observe} \proc{Reward} $r_t$,~~$S\gets\proc{Next-State}~S'$
%\zi			\kw{choose} $\proc{Random-Action}~a_t$ \kw{from} \proc{Probability} $\epsilon$
\zi	\> 			\kw{update} $p$ \kw{with} normalized-\proc{Behavior-Vector}
%\zi				\kw{update} \proc{Behavior-Vector} from collabortor's behavior
\zi	\> 		\kw{sample} $\proc{Minibatch}<\tilde{S},g,\tilde{r},\tilde{S'},\tilde{\gamma}>$ \kw{from} $D$
\zi \> 		$y=\tilde{r}+\tilde{\gamma}\cdot\max Q_{\theta}\left(\tilde{S'}, \argmax\limits_{g} Q_{\theta^{-}}(\tilde{S},g)\right)$
\zi \> 		\kw{perform} SGD \kw{on} $[y-Q_{\theta}(\tilde{S},g)]^2$ with respect to $\theta$
\zi \> 		\kw{reset} $\hat{Q}=Q$ every $C$ steps
			\End
\zi	\> 	\kw{resample} \kw{from} $p$ every $C'$ steps
		\End
\end{codebox}

\end{center}
\end{multicols}

\end{document}
